#Time Step for LSTM Layer
for pair_text_idx, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):

    for timestep, word in enumerate(input_text):
        encoder_input_data[pair_text_idx, timestep, inverse_input_vocab[word]] = 1.
    # decoder_target_data is ahead of decoder_input_data by one timestep
    for timestep, word in enumerate(target_text):
        decoder_input_data[pair_text_idx, timestep, inverse_target_vocab[word]] = 1.
        if timestep > 0:
            # decoder_target_data will be ahead by one timestep（LSTM は前タイムステップの隠れ状態を現タイムステップの隠れ状態に使う）
            # decoder_target_data will not include the start character.
            decoder_target_data[pair_text_idx, timestep - 1, inverse_target_vocab[word]] = 1.

NUM_HIDDEN_UNITS = 256 # NUM_HIDDEN_LAYERS
BATCH_SIZE = 64
NUM_EPOCHS = 100

#Encoder Architecture
encoder_inputs = Input(shape=(None, encoder_vocab_size))
encoder_lstm = LSTM(units=NUM_HIDDEN_UNITS, return_state=True)
# x-axis: time-step lstm
encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)
encoder_states = [state_h, state_c] # We discard `encoder_outputs` and only keep the states.

#Decoder Architecture
# We set up our decoder to return full output sequences,
# and to return internal states as well. We don't use the
# return states in the training model, but we will use them in inference.
decoder_inputs = Input(shape=(None, decoder_vocab_size))
decoder_lstm = LSTM(units=NUM_HIDDEN_UNITS, return_sequences=True, return_state=True)
# x-axis: time-step lstm
decoder_outputs, de_state_h, de_state_c = decoder_lstm(decoder_inputs, initial_state=encoder_states) # Set up the decoder, using `encoder_states` as initial state.
decoder_softmax_layer = Dense(decoder_vocab_size, activation='softmax')
decoder_outputs = decoder_softmax_layer(decoder_outputs)

#Encoder-Decoder Architecture
# Define the model that will turn, `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy") # Set up model